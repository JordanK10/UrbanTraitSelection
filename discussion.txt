saveCensusDataV2.py: Data Acquisition and Initial Aggregation
Primary Goal:
The main purpose of saveCensusDataV2.py is to fetch socio-economic data, primarily household income distributions and population counts, from the U.S. Census Bureau's American Community Survey (ACS) API. It then processes this raw data by aggregating it to various predefined geographic levels (Block Group, Tract, Community, County, State) and saves the structured data for later use in the analysis pipeline.
Key Inputs:
1. Census API Key: A hardcoded api_key is used to authenticate requests to the Census API.
2. Years: A predefined list of years (e.g., 2009 through 2023) for which data is to be fetched.
Geographic Focus (Counties and States): The script is configured with specific lists of counties and their corresponding states (FIPS codes), primarily focusing on the Chicago metropolitan area and surrounding regions.
Census Variables:
income_vars: A list of specific ACS variable codes (e.g., B19001_002E to B19001_017E) representing counts of households within different income brackets.
pop_vars: A list of ACS variable codes for population counts (e.g., B01003_001E for total population).
An additional variable list seems to include NAME (for geographic unit names) and potentially other income-related variables like B19083_001E (Gini Index) and B19013_001E (Median Household Income), though the primary aggregation seems to focus on income_vars and pop_vars.
Community Mapping Files:
matched_chicago_data.csv and matched_chicagoLand_data.csv: These CSV files are crucial. They provide the mapping between Census Tract GEOIDs and community area names, which are essential for aggregating data to the 'community' (cm) level, a custom aggregation level for the Chicago area.
Core Functionality and Workflow:
Configuration: Defines global constants like API key, years, target income/population variables, and the geographic scope (counties/states).
2. fetchIncomeData(variable, y=False):
Community Mapping Loading: This function begins by loading and processing the community mapping CSV files. It standardizes GEOIDs and community names.
Iterating Through Years and Geographies: It iterates through each specified year and then through each state and county pair.
API Data Fetching:
For each year/county, it constructs a URL to query the ACS API. It prioritizes fetching data at the Block Group (bg) level.
If the Block Group data fetch fails (e.g., due to API error, no data available for that year/geography), it implements a fallback mechanism to fetch data at the Census Tract (tr) level for that county.
The fetched data (JSON from API) is converted into pandas DataFrames.
Determining Input Level for Aggregation: Based on the success of BG or TR level fetches for a given year across all counties, it determines the most granular level available (bg if any BG data was successfully fetched, otherwise tr if any TR data was fetched). If no data is fetched, the level is 'none'.
Calling Aggregation: It passes the list of successfully fetched DataFrames (either all BG or all TR for that year) and the determined input level to the genAggregatedDFs function.
Storing Results: The aggregated data returned by genAggregatedDFs for each year is stored in a dictionary datas, keyed by year, where each value is another dictionary keyed by aggregation level (e.g., datas[2014]['ct'] would hold the county-level DataFrame for 2014).
genAggregatedDFs(aggs, data_to_aggregate, input_level, variables, year, community_mapping_df):
Purpose: This is the workhorse for aggregating the fetched data upwards through the hierarchy: bg -> tr -> cm -> ct -> st.
Initial Processing: Concatenates input DataFrames, converts data to numeric, and filters out rows with zero sums for the specified variables.
Identifier Preparation: Creates a temp_tract_id (concatenation of state, county, and tract FIPS) for linking and grouping. It also uses the provided community_mapping_df to associate tracts with community areas.
Handling Input Level:
If input_level is 'bg': It sets up Block Group level data, performs an initial tract-to-community mapping alignment using matchTracts (if necessary), merges community names, and then aggregates BG data to the Tract level (df_tract).
If input_level is 'tr': The input data is already at the Tract level (df_tract), so it merges community names directly.
Hierarchical Aggregation:
Tract -> Community (cm): Aggregates df_tract to the community level by summing numeric variables, grouping by the 'community' column. It then merges back state/county identifiers for each community.
Tract -> County (ct): Aggregates df_tract to the county level, grouping by 'county' (and 'state').
County -> State (st): Aggregates df_county to the state level, grouping by 'state'.
Return: Returns a tuple of DataFrames (one for each successfully aggregated level, starting from the input level) and a list of the corresponding aggregation level names.
matchTracts(df_tract, comms, identifiers=False):
Purpose: This utility function appears designed to reconcile tract definitions. It seems to handle cases where some tracts in the census data might not directly match tracts in the community mapping file (comms).
It identifies "leftover" tracts (those in df_tract not perfectly matching comms based on tract and county codes).
For these leftovers, it attempts a fuzzy match by grouping based on the first 4 digits of the tract code and summing their data, then appending "00" to create a 6-digit tract code. This is likely to handle historical changes or variations in tract numbering (e.g., split tracts).
save_data(data, filename): A simple utility to save the final datas dictionary (containing all aggregated data for all years and levels) to a pickle file.
main():
Calls fetchIncomeData to get the data.
Calls save_data to store the results.
Outputs:
Primary Output: A pickle file named data/census_data1.pkl. This file contains a nested dictionary structure:
Outer keys: Years (e.g., 2009, 2010, ..., 2023).
Inner keys: Aggregation levels (e.g., 'bg', 'tr', 'cm', 'ct', 'st').
Values: pandas DataFrames containing the aggregated income distribution counts, population counts, and relevant geographic identifiers for that year and aggregation level.
Console Output: Detailed print statements about the data fetching and aggregation process, including successes, warnings, and errors.
In essence, saveCensusDataV2.py is the foundational data acquisition and preparation step. It navigates the complexities of fetching data from the Census API for multiple years and geographies, handles potential missing data at finer granularities by falling back to coarser levels, and then systematically aggregates this information up a defined spatial hierarchy, creating a clean, structured dataset ready for the subsequent Price decomposition analyses.
Next, I will read procPrice2.py (which is not in the attached files, so I will assume you mean aggregatePrice.py or plotPrice.py, or if you have a file named procPrice2.py you would like me to read, please provide it or confirm its name) and provide a similar summary. Since procPrice.py is not listed, I'll assume you might be referring to a script that processes the data from saveCensusDataV2.py before the hierarchical aggregation in aggregatePrice.py. Given the README.md we just created, this would be procPrice2.py. If this is incorrect, please let me know!
Assuming you meant procPrice2.py as described in the README, and since I don't have its content, I will describe its intended purpose based on the project structure. If you have the file content, I can give a more precise summary.
---
Hypothetical procPrice2.py based on README description:
procPrice2.py (Hypothetical): Calculating Unit-Level Metrics and Initial Price Components
Primary Goal:
The hypothetical procPrice2.py would be responsible for taking the cleaned and aggregated census data (from data/census_data1.pkl produced by saveCensusDataV2.py) and performing initial calculations for individual geographic units across various N-year periods. This script would focus on within-unit changes and calculate direct Price decomposition terms (like the covariance between an attribute and fitness within a single unit's changing internal composition, if applicable, or more likely, the components that contribute to 
Δ
Z
ˉ
Δ 
Z
ˉ
  for each unit over N years).
Key Inputs:
data/census_data1.pkl: The primary input, containing population and income distribution data per year and aggregation level.
Configuration Parameters:
Definitions of N-year periods for analysis (e.g., 1-year changes, 5-year changes, 9-year changes).
Constants for calculating average log income (e.g., income bin midpoints, which are also defined in procHelpers.py).
The list of income_vars and xbins_values_series (also from procHelpers.py or defined locally).
Core Functionality and Workflow (Speculative):
Loading Data: Loads the census_data1.pkl dictionary.
Iterating Through N-Year Periods and Aggregation Levels:
The script would likely loop through different values of N (e.g., N=1, N=5, N=9 years).
For each N, it would iterate through each aggregation level present in the input data (e.g., 'bg', 'tr', 'cm', 'ct', 'st').
Calculating Metrics for Each Unit:
For each geographic unit at a given aggregation level and for a specific N-year period (from a FirstYear to Year = FirstYear + N):
Fetch Initial and Final Data: Retrieve the unit's population and income distribution data for FirstYear and Year.
Calculate Average Log Income: Compute 
Z
ˉ
(
t
0
)
Z
ˉ
 (t 
0
​
 ) (AvgLogIncInitial) and 
Z
ˉ
(
t
N
)
Z
ˉ
 (t 
N
​
 ) (AvgLogIncFinal) for the unit using a helper function (like calculate_avg_log_income_for_row potentially from procHelpers.py or defined locally). This involves using the income bin counts and the predefined midpoints for each bin.
Calculate Total Change (ΔZ̄): Compute 
Δ
Z
ˉ
=
Z
ˉ
(
t
N
)
−
Z
ˉ
(
t
0
)
Δ 
Z
ˉ
 = 
Z
ˉ
 (t 
N
​
 )− 
Z
ˉ
 (t 
0
​
 ) for the unit.
Calculate Population Change Metrics:
Initial Population (PopInitial), Final Population (PopFinal).
Fractional Population Change (PopFracChange_N).
Calculate Price Equation Components (Direct/Internal): This is where the "Price" part of the script name comes in. Depending on the specific formulation of the Price equation being applied at this stage (e.g., if analyzing change due to shifting internal distributions within a unit that itself is composed of sub-entities not explicitly modeled in census_data1.pkl at this level, or if it's a simpler decomposition of 
Δ
Z
ˉ
Δ 
Z
ˉ
 ), it might calculate:
Covariance Term (CovTerm_N): A term reflecting the covariance between some characteristic (e.g., initial income of sub-components if available, or just related to population share changes) and fitness (e.g., population growth of sub-components).
Expectation Term (ExpTerm_N or AvgIncomeGrowth_N): A term reflecting the average change in the characteristic of the sub-components, weighted by their initial proportions and fitness.
The script might calculate two sets of these terms, one related to population decomposition (CovTerm_N, ExpPopTerm_N) and another related to income decomposition (CovIncTerm_N, ExpIncTerm_N).
It might also calculate direct covariance measures like CovDistShift_N (covariance between income and change in population share of income bins) and CovFracPopChg_N.
Sanity Checks: Include calculations to verify that the decomposed terms sum up to the total change (SanityCheckPop_N, SanityCheckInc_N).
Assembling Results: The calculated metrics for each unit, aggregation level, and N-year period would be collected into a single comprehensive pandas DataFrame. Each row would represent a unique geographic unit for a specific N-year lookback period, identified by UnitName, Agg, Year, FirstYear, and N.
Saving Output: The final DataFrame containing all these calculated metrics would be saved to a pickle file.
Outputs (Speculative):
Primary Output: A pickle file, likely named results/analysis_Price2.pkl. This DataFrame would be the main input for subsequent scripts like aggregatePrice2.py (for hierarchical aggregation) and plotPrice.py (for visualization).
Columns would include:
Identifiers: UnitName, Agg, Year, FirstYear, N.
Parent Identifiers: ParentState, ParentCounty, ParentCommunity (useful for linking in later stages).
Population data: Population (final), PopInitial, NumMembers_N (number of constituent units if applicable at this stage, or perhaps number of households).
Income data: AvgLogInc (final), AvgLogIncInitial.
Calculated changes: DeltaZbar_N (Total Avg Log Income Change).
Decomposed Price terms: PopFracChange_N, AvgIncomeGrowth_N, CovTerm_N, ExpPopTerm_N, SanityCheckPop_N, CovIncTerm_N, ExpIncTerm_N, SanityCheckInc_N, CovDistShift_N, CovFracPopChg_N.
Console Output: Logs detailing the processing steps, number of units processed, and any warnings or errors.
In essence, the hypothetical procPrice2.py would serve as the analytical engine that transforms the time-series census data into meaningful socio-economic change metrics and initial Price decomposition components for each individual geographic unit over various timeframes. This prepares the data for both visualization of unit-level trends and for the more complex hierarchical aggregation of these Price terms.
---
Now, for plotPrice.py. I will read its content and provide the summary.
Okay, I've reviewed plotPrice.py. Here's the summary:
plotPrice.py: Visualization and Summary of Unit-Level Metrics
Primary Goal:
The main purpose of plotPrice.py is to generate visualizations (KDE plots and histograms) and summary CSV files for the various socio-economic metrics and Price equation components calculated by a preceding script (presumably procPrice2.py which produces analysis_Price2.pkl). It helps in understanding the distributions of these metrics across different geographic aggregation levels (Agg) and for various N-year periods (N).
Key Inputs:
Data File (Pickle or CSV):
The script expects an input file containing a pandas DataFrame. By default, it tries to load results/analysis_Price2.pkl.
It can also take a filename as a command-line argument.
The load_data function is robust: it first tries to load as a pickle file, and if that fails (e.g., UnpicklingError), it attempts to load it as a CSV file.
This input DataFrame should contain columns for geographic identifiers (UnitName, Agg, ParentCounty, etc.), year information (Year, FirstYear), and the various calculated metrics (e.g., AvgLogInc, DeltaZbar_{n}, CovTerm_{n}, PopFracChange_{n}, where {n} is the N-year period).
Configuration (Implicit and Explicit):
target_county_to_filter: An optional hardcoded variable to filter the data for a specific county (e.g., "031" for Cook County, IL) before plotting. If None, all data is used.
metrics_to_plot_templates: A predefined list of dictionaries that specifies which columns to plot and their corresponding user-friendly labels. This list includes templates for N-specific column names (e.g., CovTerm_{n}).
Core Functionality and Workflow:
Loading Data (load_data):
Loads the input data file using the flexible pickle/CSV loading strategy.
Main Processing (main function):
Setup: Defines output directories (pricePlots for plots, results for CSVs) and ensures they exist.
Calculate N: If an 'N' column (representing the N-year period) isn't already present, it calculates it as Year - FirstYear. It filters out data where N is invalid (NaN, zero, or negative) and converts N to an integer.
Iterate Through Unique N values: The script identifies all unique N-year periods present in the data and processes each one separately.
For each N value:
Filters the main DataFrame to get data specific to the current N.
Iterate Through Metrics: Loops through the metrics_to_plot_templates list. For each metric:
Constructs the actual column name based on the current N (e.g., CovTerm_9 if N is 9).
Checks if the column exists in the DataFrame and has non-NaN data.
Generate Plots:
plot_metric_distribution_histograms(): For each aggregation level (bg, tr, cm, ct, st), this function generates a separate plot showing the distribution of the current metric. The plot includes both a Kernel Density Estimate (KDE) and an overlaid histogram. It also displays the mean and variance on the plot. X-axis limits are dynamically set based on the data's range (attempting a +/- 3 sigma range with robust quantile-based fallbacks). Plots are saved as PDFs in a subdirectory specific to the metric and N-value (and county filter if active).
plot_combined_kde_by_metric(): For the current metric, this function generates a single plot that overlays the KDEs for all aggregation levels. This allows for direct comparison of the metric's distribution across different scales. Mean lines for each aggregation level are also included. X-axis limits are primarily determined by the tract-level distribution (+/- 4 sigma) for consistency. Plots are saved as PDFs.
Both plotting functions can apply the target_county_to_filter if it's set and the ParentCounty column is available. They handle potential type mismatches for the county filter.
Save Summary CSV (save_summary_csv): After processing all metrics for the current N, this function creates a CSV file containing a subset of the data relevant to that N. It selects base identifier columns and the N-specific metric columns, and saves it to the results directory (e.g., price_analysis_summary_N9.csv).
Plotting Functions Details:
Error Handling: The plotting functions include error handling (e.g., if required columns are missing, if data is empty after filtering, or if plotting itself raises an exception).
Aesthetics: Use seaborn for KDE plots and matplotlib for histograms, applying labels, titles, legends, and grids to make plots informative.
Dynamic Limiting: Attempts to set reasonable x-axis limits for individual histograms to focus on the bulk of the distribution.
Handling Low Variance: The combined KDE plot function checks if data for an aggregation level has very low variance; if so, it plots a vertical line at the constant value instead of attempting a KDE.
Outputs:
1. PDF Plot Files:
Saved in the pricePlots/ directory.
Individual Distribution Plots: In subdirectories like histograms_distribution_DeltaZbar-N9/, files like dist_deltazbar_n9_tr.pdf.
Combined KDE Plots: In a subdirectory combined_kde_plots/, files like combined_kde_deltazbar_n9.pdf.
Filenames include the metric name, N-value, aggregation level (for individual plots), and county filter if applied.
CSV Summary Files:
Saved in the results/ directory.
Files like price_analysis_summary_N9.csv, price_analysis_summary_N1.csv, etc.
These CSVs contain the raw data for the specified N-year period, including identifiers and all the relevant N-specific calculated metrics, making them suitable for further analysis or import into other tools.
Console Output: Logs detailing the loading process, which N-values are being processed, which plots are being generated, and any warnings or errors encountered.
In essence, plotPrice.py is a crucial post-processing script that takes the quantitative outputs of the Price analysis (from procPrice2.py) and translates them into visual and tabular summaries. This aids in exploring the characteristics of the calculated socio-economic change components across different geographic scales and timeframes, and in identifying patterns or outliers.
---
Finally, let's do aggregatePrice.py. I see aggregatePrice.py in the attached files, so I will summarize that one.
You're right, the script you provided and that I have access to is aggregatePrice.py (which seems to be the same as the aggregatePrice2.py we've been working on, or a very close version of it). My apologies for any confusion if I referred to it as aggregatePrice2.py previously when discussing its current state.
Here's a summary of aggregatePrice.py based on the content you've provided:
aggregatePrice.py: Hierarchical Price Decomposition and Aggregation
Primary Goal:
The core purpose of aggregatePrice.py is to implement the hierarchical Price decomposition. It takes unit-level socio-economic data (specifically initial and final population and average log income, and the direct growth rate at the base level) and iteratively calculates and aggregates Price equation components up a defined geographic hierarchy. The ultimate aim is to decompose the total change in average log income (or growth rate) at a higher geographic level (e.g., county) into selection effects occurring at that level and the sum of effects transmitted from all lower levels in the hierarchy.
Key Inputs:
data/census_data1.pkl: Output from saveCensusDataV2.py. This provides the raw census data (population and income bin counts) for each geographic unit at various levels for different years. It's used in this script primarily to fetch initial population and calculate initial average log income for the base-level units.
results/analysis_Price2.pkl: Output from a script like procPrice2.py (as described in the README). This file is crucial as it contains the pre-calculated final population (Population) and final average log income (AvgLogInc) for each geographic unit, as well as identifiers like UnitName, Agg (aggregation level), Year, FirstYear, and N (the period of change).
Configuration Parameters:
TARGET_N: An integer specifying the N-year period for the analysis (e.g., 9 years). The script filters analysis_Price2.pkl for this specific N.
HIERARCHY_LEVELS: A list defining the order of geographic aggregation, from the lowest (base level, L_BASE_name) to the highest (e.g., ['bg', 'tr', 'cm', 'ct'] for Block Group to County).
child_to_parent_id_col_map: A dictionary that maps each child level to the column name in its DataFrame that identifies its parent unit in the next higher level (e.g., 'bg': 'TempTractID' means the TempTractID column in the block group data links BGs to their parent tracts).
Constants for income calculation: income_vars (list of income variable names from ACS) and xbins_values_series (Series mapping income variables to their representative midpoint values). These are used by calculate_avg_log_income_for_row.
Core Functionality and Workflow (Steps):
Step 0: Configuration and Setup:
Defines constants for income calculation (bin midpoints, etc.).
Sets TARGET_N, HIERARCHY_LEVELS, and child_to_parent_id_col_map.
Loads the two input pickle files (census_data_dict and analysis_df).
Filters analysis_df for the specified TARGET_N.
Step 1: Prepare Base-Level Data (prepare_base_level_data function):
Purpose: To prepare the data for the lowest level in the hierarchy ( L_BASE_name, e.g., 'bg').
Takes the analysis_df_target_n (filtered for L_BASE_name and TARGET_N) which contains final population and final average log income for these base units.
Iterates through each base unit:
Uses census_data_dict and the unit's FirstYear to look up its initial census data (income bin counts and total population).
Calculates PopInitial_{L_BASE_name}.
Calculates AvgLogIncInitial_{L_BASE_name} using calculate_avg_log_income_for_row.
Renames Population and AvgLogInc from analysis_df to PopFinal_{L_BASE_name} and AvgLogIncFinal_{L_BASE_name} respectively.
Calculates the direct change at this base level: GrowthRate_Direct_{L_BASE_name} = (AvgLogIncFinal - AvgLogIncInitial) / TARGET_N.
Adds the parent identifier column (e.g., TempTractID for BGs if they are the base) necessary for grouping at the next aggregation step. If L_BASE_name is 'bg', it constructs TempTractID from the 'UnitName'.
Selects relevant columns and filters out rows with missing essential data (populations, initial income, growth rate, parent ID) or PopInitial <= 0.
Stores the processed base-level DataFrame in level_outputs[L_BASE_name].
Step 2: Iterative Calculation Up the Hierarchy (main loop and calculate_price_terms_for_parent_unit function):
Iterates from the second level in HIERARCHY_LEVELS up to the highest level. In each iteration, the current level is current_parent_level_name, and the level below it is current_child_level_name.
A. Prepare Child Data:
Retrieves the child-level DataFrame from level_outputs.
Identifies the parent_id_col used to group these children under their respective parents.
Renames columns in the child DataFrame to generic names (e.g., PopInitial_bg becomes Child_PopInitial) that the calculate_price_terms_for_parent_unit function expects. This includes:
PopInitial, PopFinal, AvgLogIncInitial.
The input for transmission: GrowthRate_Direct_{child} if child is base, or Total_GrowthRate_{child} if child is an intermediate level. This becomes Child_GrowthRate_Input_For_Transmission.
Selection and transmitted terms from levels below the child (if any) are also renamed (e.g., Sel_Rate_At_This_Level_tr becomes Child_Sel_Rate_At_Child_Level when 'tr' is the child).
Calculates child fitness: Child_w_C = Child_PopFinal / Child_PopInitial.
Drops rows with missing essential data for the upcoming calculation.
B. Group by Parent ID and Apply Calculation:
Groups the prepared child DataFrame by parent_id_col.
Applies the calculate_price_terms_for_parent_unit function to each group (i.e., to all children belonging to a single parent unit).
calculate_price_terms_for_parent_unit(child_group_df, ...) function:
This function takes a DataFrame (child_group_df) containing all children of a single parent unit.
Calculates p_C: The initial population proportion of each child within the parent group.
Aggregates Initial Traits: Calculates PopInitial, PopFinal, and AvgLogIncInitial for the current_parent_level_name by summing/averaging child data.
Calculates w_bar_Children_Agg: The average fitness of children for this parent (
w
ˉ
P
=
∑
p
C
w
C
w
ˉ
  
P
​
 =∑p 
C
​
 w 
C
​
 ).
Calculates Price Terms (if \( \bar{w}P \) is valid):
Selection at Parent Level (Sel_Rate_At_This_Level_{parent}): 
1
N
∑
p
C
(
w
C
w
ˉ
P
−
1
)
(
Z
C
,
t
0
−
Z
ˉ
P
,
t
0
)
N
1
​
 ∑p 
C
​
 ( 
w
ˉ
  
P
​
 
w 
C
​
 
​
 −1)(Z 
C,t 
0
​
 
​
 − 
Z
ˉ
  
P,t 
0
​
 
​
 ). This is the selection effect occurring directly among the children within this parent unit.
Transmitted Selection Terms: For each level below the current_child_level_name (i.e., "grandchild" levels or lower relative to the current parent), it takes the selection terms that were already calculated at those lower levels and transmitted up to the child (e.g., Child_Transmitted_Sel_Rate_From_bg), weights them by 
p
C
w
C
w
ˉ
P
p 
C
​
  
w
ˉ
  
P
​
 
w 
C
​
 
​
 , sums them, and stores them as Transmitted_Sel_Rate_From_{lower_level}_To_{parent}. It also transmits the child's own selection term (Child_Sel_Rate_At_Child_Level).
Transmitted Base Growth Rate (Transmitted_GrowthRate_From_{L_BASE_name}_To_{parent}): Takes the Child_GrowthRate_Input_For_Transmission (if child is base) or Child_Transmitted_GrowthRate_From_Base (if child is intermediate), weights it by 
p
C
w
C
w
ˉ
P
p 
C
​
  
w
ˉ
  
P
​
 
w 
C
​
 
​
 , sums it, and stores it. This represents the portion of the growth rate from the very bottom of the hierarchy that is transmitted up to this parent.
Calculates Total_GrowthRate_{parent}: Sums all the calculated selection and transmitted rate terms for this parent.
Carries Over Grandparent Identifiers: Copies necessary parent identifiers (e.g., ParentState for a county) from the child data to the parent data.
Returns a pandas Series with all these calculated terms for the parent unit.
C. Store Results:
The results from the apply operation (a DataFrame of parent units with their calculated Price terms) are processed (e.g., reset_index, rename grouping column to UnitName, add Agg column) and stored in level_outputs[current_parent_level_name].
Step 3: Final Assembly and Sanity Check:
Focuses on the DataFrame for the highest level in HIERARCHY_LEVELS (e.g., 'ct').
For each unit in this final level DataFrame:
Sums the following components:
Sel_Rate_At_This_Level_{final_parent_level_name} (selection at the final level itself).
All Transmitted_Sel_Rate_From_{intermediate_level}_To_{final_parent_level_name} terms (selection transmitted from all intermediate levels).
Transmitted_GrowthRate_From_{base_level_name}_To_{final_parent_level_name} (growth rate transmitted from the very base level).
Compares this component_sum to the Total_GrowthRate_{final_parent_level_name} calculated for that unit.
Prints whether the sanity check passes (i.e., if the sum of parts approximately equals the calculated total).
Step 4: Save Outputs:
Creates an results/ directory if it doesn't exist.
Saves the entire level_outputs dictionary (containing DataFrames for all processed levels) to a pickle file (e.g., hierarchical_price_decomposition_N9_upto_ct_all_levels.pkl).
Saves the DataFrame for the final aggregation level (e.g., county summary) to a CSV file (e.g., hierarchical_price_decomposition_N9_CT_summary.csv), setting UnitName as the index.
Outputs:
Pickle File: results/hierarchical_price_decomposition_N{TARGET_N}_upto_{final_level_name}_all_levels.pkl
Contains the level_outputs dictionary. Each key in this dictionary is an aggregation level name (e.g., 'bg', 'tr', 'cm', 'ct'), and the value is a pandas DataFrame.
The DataFrame for L_BASE_name contains initial/final populations, initial/final average log incomes, and the direct growth rate.
DataFrames for higher levels contain their aggregated initial/final populations and average log incomes, their own selection rate term (Sel_Rate_At_This_Level_...), various transmitted selection rate terms from lower levels (Transmitted_Sel_Rate_From_..._To_...), the transmitted growth rate from the base level (Transmitted_GrowthRate_From_{L_BASE_name}_To_...), and the Total_GrowthRate_... for that parent unit.
CSV File: results/hierarchical_price_decomposition_N{TARGET_N}_{final_level_name.upper()}_summary.csv
A CSV representation of the DataFrame for the highest aggregation level specified in HIERARCHY_LEVELS. This provides a summarized, easily viewable output of the full decomposition for the top-level units.
Console Output: Detailed logs of each step, including data shapes, column names, sample data at each aggregation level, and the results of the sanity checks.
In essence, aggregatePrice.py is the capstone script of the hierarchical decomposition. It meticulously builds up the Price components from the most granular geographic level to the desired highest level, allowing for a detailed attribution of change in average log income (growth rate) to selection and transmission effects across the spatial hierarchy. It translates the complex Price theory into a concrete, step-by-step calculation across nested geographic units.